## 定义

一致最小方差无偏估计是指在该估计量的所有无偏估计中，选择一个方差最小的估计，也被称为UMVUE。

无偏估计是指期望等于真实值的估计。

UMVUE还有很多好的性质，如与零的估计量独立等等，它是基于均方误差最小的原则提出的，不过这里我们没有必要多做介绍，如有兴趣者请参见《概率论与数理统计》（茆诗松）一书。 ~~（笑死，原教材根本看不懂好吗）~~ 

## Fisher信息量

$$
I(\theta) = E_{\theta}[\frac{\partial ln(p(x;\theta))}{\partial \theta} ]^2
$$



上式在统计学中被称为Fisher信息量，是统计学的基本定义之一，其大小意味着该总体分布中包含的关于未知参数 $\theta$ 的信息的多寡。

它的存在性与合理性需要以下条件的保障：

1. 参数空间 $\theta$ 是直线上的一个开区间；
2. 支撑 $S={x:p(x;\theta)>0}$ 与 $\theta$ 无关；
3. 倒数 $\frac{\partial}{\partial \theta} p(x;\theta)$ 对一切 $\theta \in \Theta$ 都存在；
4. 对 $p(x;\theta)$ ,积分与微分运算可交换次序；
5. 它本身存在。 

## C-R不等式

C-R不等式指出，当一个无偏估计求期望过程中的微分可以换到积分号（离散情况下是求和号）之内进行时，它（指这个无偏估计）的方差的下界为


$$
\frac{[g'(\theta)]^2}{nI(\theta)}
$$


当估计量为 $\theta$ 本身时，有个更好看的结果：


$$
\frac{1}{nI(\theta)}
$$


相当厉害的结果对吧 ~~（笑死，你以为老师会考你怎么通过这个不等式求UMVUE吗，这题的难点跟这知识点半毛钱的关系都没有好吗）~~ 

