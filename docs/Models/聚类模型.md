# 聚类模型


聚类是将样本划分为由类似的对象组成的多个类的过程，且分类类别是未知的。我们主要讲述K-means算法，K-means++算法，系统（层次）聚类以及DBSCAN四种算法，并对算法实现过程中的基础知识做简单讲解。

##  知识储备

###  标准化

 如果遇到数据量纲不一致的问题，那么计算距离就没有意义了，此时需要叫数据标准化，即
$$
z_i=\frac{x_i-x^-}{\sigma_x}
$$
### 样品与样品之间的常用距离

样品i与样品j之间的距离（共有p个指标）

绝对值距离(一般适用于网状结构)
$$
d（x_i,x_j)=\sum\limits_{k=1}^{p}|x_{ik}-x_{jk}|
$$
欧氏距离（最常用）
$$
d（x_i,x_j)=\sqrt{\sum\limits_{k=1}^{p}(x_{ik}-y_{jk})^2}
$$
Minkowski距离(欧氏距离的一般形式)
$$
d（x_i,x_j)=[\sum\limits_{k=1}^{p}(x_{ik}-y_{jk})^q]^\frac{1}{q}
$$
Chebyshev距离
$$
d（x_i,x_j)=\max\limits_{1\leq{k}\leq{p}}|x_{ik}-x_{jk}|
$$
马氏距离(其中后一项为样本的协方差 )
$$
d（x_i,x_j)=(x_i-x_j)' \sum^{-1}(x_i-x_j)
$$
### 指标距离

相关系数
$$
\rho(x_i,x_j)=\frac{\sum\limits_{k=1}^{p}(x_{kj}-x_j^-)(x_{ki}-x_i^-)}{\sqrt{\sum\limits_{k=1}^{p}(x_{kj}-x_j^-)^2\sum\limits_{k=1}^{p}(x_{ki}-x_i^-)^2}}
$$
夹角余弦
$$
r(x_i,x_j)=\frac{\sum\limits_{k=1}^{p}x_{kj}x_{ki}}{\sqrt{\sum\limits_{k=1}^{p}x_{kj}^2\sum\limits_{k=1}^{p}x_{ki}^2}}
$$

### 类与类之间的距离

最长（短）距离法
$$
D(G_p,G_q)=\max/\min d(x_i,x_j)
$$
组间平均连接法以及组内平均连接法（常用）



<img src="C:\Users\LENOVO\Desktop\屏幕截图 2021-08-12 000704.png" alt="屏幕截图 2021-08-12 000704" style="zoom: 67%;" />

<img src="C:\Users\LENOVO\Desktop\屏幕截图 2021-08-12 000639.png" alt="屏幕截图 2021-08-12 000639" style="zoom: 67%;" />

重心法(常用)
$$
D(G_p,G_q)=d(x_p^-,x_q^-)
$$


## 模型简述

###  K-means算法

####  算法描述

 确定聚类个数K以及最大迭代次数n（10次左右）；

 随机选择K个点作为初始的聚类中心（不一定为样本点）；

 分配各数据对象到距离最近的类中；

 寻找该类的中心点并将其更新为该类的聚类中心；

 循环上述两个步骤，至聚类中心不变或达到迭代次数停止。

####  优缺点

 优点：

 （1）简单快速

 （2）对于大数据集，效率较高

 缺点：

 （1）实现要给定K

 （2）对初始选择的K个聚类中心较敏感

 （3）对孤立点数据集敏感

###  K-means++算法

 基本原则：初始的聚类中心之间的距离尽可能的远。

####  算法描述

 随机选取一个样本点作为第一个聚类中心；

 计算每个样本与当前已有聚类中心的最短距离，这个值越大，被选作聚类中心的概率也就越大，根据概率大小（轮盘法）选出下一个聚类中心；

 重复上述步骤，直到算出K个聚类中心作为初始聚类中心后，继续使用，K-means算法。

####  优缺点

 克服了K-means算法中的（2）、（3）问题，但是仍需要首先确定聚类个数。

###  层次（系统）算法

#### 算法描述

<img src="C:\Users\LENOVO\Desktop\屏幕截图 2021-08-12 092240.png" alt="屏幕截图 2021-08-12 092240" style="zoom:50%;" />

说明：

对于步骤（3）,不同的距离计算方法，会得到不同的聚类结果。

对于步骤（6），可根据聚类谱系图，最终根据需求确定分类个数即类

### K值的确定

肘部法则：通过图形大致估计出最优的聚类数量

各个类的畸变程度等于该类重心与其内部样本位置距离的平方和
$$
\sum\limits_{i\in{C_k}}|x_i-u_k|^2\\其中C_k表示第k个类，类的重心位置记为u_k
$$
定义聚合系数J为所有类的畸变程度之和
$$
J=\sum\limits_{k=1}^K\sum\limits_{i\in{C_k}}|x_i-u_k|^2
$$
画出聚合系数折线图，以聚类的类别数K为横坐标，聚类系数J为纵坐标,K越大，J越小，转折点对应的K值即为聚类个数。

### 几点说明

根据分类目的选取指标，指标选取不同，聚类结果一般不同

距离定义方式不同，聚类结果一般不同

聚类方法不同，聚类结果一般不同（我们只是根据样本特征，对样本进行分类，背后没有清楚的逻辑，解释的通就好）

注意指标的量纲，量纲不同时，注意将其标准化

聚类分析的结果可能不令人满意，因为我们所做的是一个数学处理，只需要找到一个合理的解释即可

### DBSCAN算法

DBSCAN算法是一种具有噪声的基于密度的聚类方法，能在具有噪声的空间数据库中发现任意形状的簇，可将密度足够大的相邻区域连接，能有效处理异常数据。

#### 算法内容

遍历每一个样本点，并将样本点分为三类

核心点：在半径Eps内含有不少于MinPts数目的点

边界点：在半径Eps内点的数量小于MinPts，但落在核心点的邻域内

噪音点：既不是核心点，也不是边界点的点

核心点与边界点均算在同一簇内

#### 优缺点

优点

基于密度定义，能处理任意形状和大小的簇

可在聚类的同时发现异常点

与K-means比起来，不需要指定K

缺点

对于Eps与MinPts敏感

当聚类密度不均匀时，聚类距离相差很大时，聚类质量差

当数据量大时，计算密度单元的计算复杂度大

##  模型代码

### K-means算法

```python
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

## 计算欧拉距离
def calcDis(dataSet, centroids, k):
    clalist=[]
    for data in dataSet:
        diff = np.tile(data, (k, 1)) - centroids  #相减   (np.tile(a,(2,1))就是把a先沿x轴复制1倍，即没有复制，仍然是 [0,1,2]。 再把结果沿y方向复制2倍得到array([[0,1,2],[0,1,2]]))
        squaredDiff = diff ** 2     #平方
        squaredDist = np.sum(squaredDiff, axis=1)   #和  (axis=1表示行)
        distance = squaredDist ** 0.5  #开根号
        clalist.append(distance) 
    clalist = np.array(clalist)  #返回一个每个点到质点的距离len(dateSet)*k的数组
    return clalist

## 计算质心
def classify(dataSet, centroids, k):
    ## 计算样本到质心的距离
    clalist = calcDis(dataSet, centroids, k)
    ## 分组并计算新的质心
    minDistIndices = np.argmin(clalist, axis=1)    #axis=1 表示求出每行的最小值的下标
    newCentroids = pd.DataFrame(dataSet).groupby(minDistIndices).mean() #DataFramte(dataSet)对DataSet分组，groupby(min)按照min进行统计分类，mean()对分类结果求均值
    newCentroids = newCentroids.values
 
    ## 计算变化量
    changed = newCentroids - centroids
 
    return changed, newCentroids

## 使用k-means分类
def kmeans(dataSet, k):
    ## 随机取质心
    centroids = random.sample(dataSet, k)
    
    ## 更新质心 直到变化量全为0
    changed, newCentroids = classify(dataSet, centroids, k)
    while np.any(changed != 0):
        changed, newCentroids = classify(dataSet, newCentroids, k)
 
    centroids = sorted(newCentroids.tolist())   #tolist()将矩阵转换成列表 sorted()排序
 
    ## 根据质心计算每个集群
    cluster = []
    clalist = calcDis(dataSet, centroids, k) #调用欧拉距离
    minDistIndices = np.argmin(clalist, axis=1)  
    for i in range(k):
        cluster.append([])
    for i, j in enumerate(minDistIndices):   #enymerate()可同时遍历索引和遍历元素
        cluster[j].append(dataSet[i])
        
    return centroids, cluster
 
## 创建数据集
def createDataSet():
    return [[1, 1], [1, 2], [2, 1], [6, 4], [6, 3], [5, 4]]

if __name__=='__main__': 
    dataset = createDataSet()
    centroids, cluster = kmeans(dataset, 2)
    print('质心为：%s' % centroids)
    print('集群为：%s' % cluster)
    for i in range(len(dataset)):
      plt.scatter(dataset[i][0],dataset[i][1], marker = 'o',color = 'green', s = 40 ,label = '原始点') ##  记号形状       颜色      点的大小      设置标签
      for j in range(len(centroids)):
        plt.scatter(centroids[j][0],centroids[j][1],marker='x',color='red',s=50,label='质心')
        plt.show
```

### K-means++算法

```python
def get_cent(points, k):
    '''
    kmeans++的初始化聚类中心的方法
    :param points: 样本
    :param k: 聚类中心的个数
    :return: 初始化后的聚类中心
    '''
    m, n = np.shape(points)
    cluster_centers = np.mat(np.zeros((k, n)))
 
    ## 1、随机选择一个样本点作为第一个聚类中心
    index = np.random.randint(0, m)
    cluster_centers[0, ] = np.copy(points[index, ])  ## 复制函数，修改cluster_centers，不会影响points
 
    ## 2、初始化一个距离序列
    d = [0.0 for _ in range(m)]
 
    for i in range(1, k):
        sum_all = 0
        for j in range(m):
            ## 3、对每一个样本找到最近的聚类中心点
            d[j] = nearest(points[j, ], cluster_centers[0:i, ])
            ## 4、将所有的最短距离相加
            sum_all += d[j]
        ## 5、取得sum_all之间的随机值
        sum_all *= random()
        ## 6、获得距离最远的样本点作为聚类中心点
        for j, di in enumerate(d):  ## enumerate()函数用于将一个可遍历的数据对象（如列表、元组或字符串）组合为一个索引序列，同事列出数据和数据下标一般用在for循环中
            sum_all -= di
            if sum_all > 0:
                continue
            cluster_centers[i] = np.copy(points[j, ])
            break
    return cluster_centers
```
在上述代码中，函数nearest()用于计算最短距离，具体实现过程如下所示：

```python
def nearest(point, cluster_centers):
    '''
    计算point和cluster_centers之间的最小距离
    :param point: 当前的样本点
    :param cluster_centers: 当前已经初始化的聚类中心
    :return: 返回point与当前聚类中心的最短距离
    '''
    min_dist = FLOAT_MAX
    m = np.shape(cluster_centers)[0]  ## 当前已经初始化聚类中心的个数
    for i in range(m):
        ## 计算point与每个聚类中心之间的距离
        d = distance(point, cluster_centers[i, ])
        ## 选择最短距离
        if min_dist > d:
            min_dist = d
    return min_dist
```

### 层次（系统）聚类

```python

```

